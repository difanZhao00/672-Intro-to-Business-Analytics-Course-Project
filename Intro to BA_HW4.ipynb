{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjdboEROkjAa"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import os\n",
        "import requests\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn import linear_model\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "from sklearn import neighbors\n",
        "from sklearn import pipeline\n",
        "from sklearn import preprocessing\n",
        "from sklearn import tree\n",
        "\n",
        "# set random_state\n",
        "random_state = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# a)\n",
        "50  points)  After  exploring  the  data,  build  numeric  prediction  models  that  predict  Spending.  Use linear regression, k-NN, and regression tree techniques. Briefly discuss the models you have built. Use cross-validation with 10 folds to estimate the generalization performance. Present the results for each of the three techniques and discuss which one yields the best performance. \n",
        "\n",
        "part a is worth 50 points in total:\n",
        "* 15 points for exploring the data (i.e., descriptive statistics including min max mean and stdv, visualizations, target \n",
        "variable distribution)\n",
        "* 10 points for correctly building linear regression model - provide screenshots and explain what you are doing and \n",
        "the corresponding results \n",
        "* 10  points  for  correctly  building  k-NN  model  -  provide  screenshots  and  explain  what  you  are  doing  and  the \n",
        "corresponding results \n",
        "* 10 points for correctly building regression tree model - provide screenshots and explain what you are doing and the \n",
        "corresponding results \n",
        "* 5 points for discussing which of the three models yields the best performance"
      ],
      "metadata": {
        "id": "ZwZQd0Rik0e1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Exploration"
      ],
      "metadata": {
        "id": "13Z9dvsCltVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create file path\n",
        "path = os.getcwd()\n",
        "f = 'HW4.xlsx'\n",
        "f_path = os.path.join(path, f)\n",
        "\n",
        "# import data\n",
        "data = pd.read_excel(f_path)\n",
        "\n",
        "# set index\n",
        "data = data.set_index(data.sequence_number)\n",
        "data.drop('sequence_number', axis=1, inplace=True)\n",
        "\n",
        "# rename columns for easy of use\n",
        "col_dict = {'Web order': 'web_order', 'Gender=male': 'gender'}\n",
        "data.rename(col_dict, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "5Lxp0luRMIlh",
        "outputId": "4112dd58-7f7d-49fa-e9e7-3dcc4ae1d39a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e57e5be0729b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# import data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# set index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1190\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                 ext = inspect_excel_format(\n\u001b[0;32m-> 1192\u001b[0;31m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1193\u001b[0m                 )\n\u001b[1;32m   1194\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mext\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m     with get_handle(\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m     ) as handle:\n\u001b[1;32m   1073\u001b[0m         \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/HW4.xlsx'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data head\n",
        "data.head()"
      ],
      "metadata": {
        "id": "66lYEXlNlrXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data info\n",
        "data.info()"
      ],
      "metadata": {
        "id": "Ef0o9yMKmyp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Categorical Variables"
      ],
      "metadata": {
        "id": "Pi0wyGP5QDXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_bar_plot(col, data=data):\n",
        "  plt.rcParams[\"figure.figsize\"] = (20,3)\n",
        "  data[col].value_counts().plot(kind='barh', xlabel = 'value', ylabel='count', title = '{} count'.format(col))\n",
        "  plt.xlim([0, 2000])\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Y5zzXRK7C0Bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_bar_plot('US')"
      ],
      "metadata": {
        "id": "ety1yD9hC8fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_bar_plot('web_order')"
      ],
      "metadata": {
        "id": "iKbJRiAPDB3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_bar_plot('gender')"
      ],
      "metadata": {
        "id": "SEcrKyyKDB-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_bar_plot('Address_is_res')"
      ],
      "metadata": {
        "id": "ExYLsHJGDCJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_bar_plot('Purchase')"
      ],
      "metadata": {
        "id": "gA3GpxldDFzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Categorical Insights:**\n",
        "* mainly US\n",
        "* mainly commercial\n",
        "* ~even split by web_order\n",
        "* ~even split by gender\n",
        "* ~even split by purchase"
      ],
      "metadata": {
        "id": "k7a0yIwULuLM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantitative Variables"
      ],
      "metadata": {
        "id": "KlqxeFIaQDdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# quantitative columns\n",
        "quant_cols = [\n",
        "    'Freq',\n",
        "    'last_update_days_ago',\n",
        "    '1st_update_days_ago'\n",
        "]"
      ],
      "metadata": {
        "id": "HRh8lAKWQSg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# describe the quantitative columns\n",
        "data[quant_cols].describe()"
      ],
      "metadata": {
        "id": "d-vOgc6MSpVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make a bar plot for every quant column\n",
        "for col in quant_cols:\n",
        "  data[col].plot(kind='box', vert=False, title = '{} distribution'.format(col))\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "YvdCYVAUSKgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# correlation between quant columns\n",
        "corr = data[quant_cols].corr()\n",
        "corr.style.background_gradient(cmap='gray')"
      ],
      "metadata": {
        "id": "EAsZERjqm2bA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Target Variable"
      ],
      "metadata": {
        "id": "2MpzZu2MRprw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# box plot for target variable\n",
        "data['Spending'].plot(kind='box', vert=False, title = '{} distribution'.format('Spending'))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TBNR3hCNRryl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# histogram with bins=30 to show distribution\n",
        "plt.hist(data['Spending'], edgecolor=\"black\", bins=40)\n",
        "plt.title(\"Spending Distribution\")\n",
        "plt.xlabel(\"Spending\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PYKcBEQSVofU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression"
      ],
      "metadata": {
        "id": "peBrS54Nlu-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split into X and y\n",
        "target = 'Spending'\n",
        "X = data.drop(target, axis=1).copy()\n",
        "y = data[target].copy()\n",
        "\n",
        "# set our scoring metric\n",
        "scoring = 'neg_root_mean_squared_error'"
      ],
      "metadata": {
        "id": "fRW_JcFJlvz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build a basic linear model\n",
        "clf_lm = linear_model.LinearRegression(n_jobs=-1)\n",
        "\n",
        "# 10 fold CV to assess generalization performance\n",
        "scores = model_selection.cross_val_score(clf_lm,\n",
        "    X,\n",
        "    y,\n",
        "    scoring=scoring,\n",
        "    cv=10\n",
        ")\n",
        "\n",
        "print(\"{} performance: {} (+/- {})\".format(scoring, round(scores.mean(), 2), round(scores.std(), 2)))"
      ],
      "metadata": {
        "id": "zSt4EvTYZsH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## k-NN"
      ],
      "metadata": {
        "id": "BgALTWO9lv_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scale the data\n",
        "scaler = preprocessing.StandardScaler()\n",
        "scaler.fit(X)\n",
        "X_std = scaler.transform(X)"
      ],
      "metadata": {
        "id": "1QMUbZNElwH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build a bassic kNN regressor with k=5\n",
        "clf_knn = neighbors.KNeighborsRegressor(\n",
        "    n_neighbors=5,\n",
        "    p=2,                     \n",
        "    metric='minkowski',\n",
        "    n_jobs=-1)\n",
        "\n",
        "# 10 fold CV to assess generalization performance\n",
        "# use the scaled data\n",
        "scores = model_selection.cross_val_score(clf_knn,\n",
        "    X_std,\n",
        "    y,\n",
        "    scoring=scoring,\n",
        "    cv=10\n",
        ")\n",
        "\n",
        "print(\"{} performance: {} (+/- {})\".format(scoring, round(scores.mean(), 2), round(scores.std(), 2)))"
      ],
      "metadata": {
        "id": "24BmR6DFLCeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression Tree"
      ],
      "metadata": {
        "id": "cuoRI-pPl1zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build a basic regression tree\n",
        "clf_tree = tree.DecisionTreeRegressor(criterion='squared_error', \n",
        "                                      max_depth=3,\n",
        "                                      random_state=random_state)\n",
        "\n",
        "# 10 fold CV to assess generalization performance\n",
        "scores = model_selection.cross_val_score(clf_tree,\n",
        "    X,\n",
        "    y,\n",
        "    scoring=scoring,\n",
        "    cv=10\n",
        ")\n",
        "\n",
        "print(\"{} performance: {} (+/- {})\".format(scoring, round(scores.mean(), 2), round(scores.std(), 2)))"
      ],
      "metadata": {
        "id": "WXglxVmrl18D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modeling Discussion\n",
        "We chose we to use **negative_root_mean_square_error** as our scoring metric, because it makes sense to penalize larger errors in this context. The negative is required by sklearn; we interpret errors closer to zero as being better.\n",
        "\n",
        "Given our scoring metric and 10 fold cv, we determined that the **Logistic Regression** model has the best initial generalization performance both in terms of bias (-119.96) and variance (25.89)."
      ],
      "metadata": {
        "id": "TGJHsergl4-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# b)\n",
        "50  points)  Engage  in  feature  engineering  (i.e.,  create  new  features  based  on  existing  features)  to optimize  the  performance  of  linear  regression,  k-NN,  and  regression  tree  techniques.  Present  the \n",
        "results for each of the three techniques (choose the best performing model for each technique in case you  try  multiple  models)  and  discuss  which  of  the  three  yields  the  best  performance.  Use  cross-validation  with  10  folds  to  estimate  the  generalization  performance.  Discuss  whether  and  why  the generalization performance was improved or not.\n",
        "\n",
        "part b is worth 50 points in total:\n",
        "* 10 points for correctly building the new linear regression model and improving the performance as much as possible \n",
        "  * provide screenshots and explain what you are doing and the corresponding results \n",
        "* 10 points for correctly building the new k-NN model and improving the performance as much as possible - provide \n",
        "screenshots and explain what you are doing and the corresponding results \n",
        "* 10 points for correctly building the new regression tree model and improving the performance as much as possible - \n",
        "provide screenshots and explain what you are doing and the corresponding results \n",
        "* 20 points for discussing if the generalization performance was improved or not for each of the techniques (linear \n",
        "regression, kNN, and regression tree) and justifying why it was improved or alternatively why it was not improved"
      ],
      "metadata": {
        "id": "Njd17Ci0k7LO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering"
      ],
      "metadata": {
        "id": "ow9m_TDuSLPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(data['Freq'], edgecolor=\"red\", bins=30)\n",
        "plt.title('Freq Distribution')\n",
        "plt.xlabel('frequency')\n",
        "plt.ylabel('cnt')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-ksXBrFMVf0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# manual feature engineering\n",
        "X_tf = X.copy()\n",
        "\n",
        "# frequent shopper\n",
        "X_tf['frequent_shopper'] = X_tf['Freq'].apply(lambda x: 1 if x >= 3 else 0)\n",
        "\n",
        "# days b/w first and last update\n",
        "X_tf['days_between_update'] = X_tf['1st_update_days_ago'] - X_tf['last_update_days_ago'] \n",
        "\n",
        "# female shopping and shipping to residential address\n",
        "X_tf['female_shopping_residential'] = (1-X_tf['gender']) + X_tf['Address_is_res']\n",
        "\n",
        "# we think Freq is a) skewed and b) can be modelled as Poisson process so we take the\n",
        "X_tf['sq_rt_Freq'] = X_tf['Freq']**(1/2)\n",
        "\n",
        "# all three together\n",
        "X_tf['all_three'] = X_tf['Freq'] * X_tf['last_update_days_ago'] * X_tf['1st_update_days_ago']"
      ],
      "metadata": {
        "id": "F1o3s-qqSKYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# automated feature engineering\n",
        "for col in quant_cols:\n",
        "  col2 = col + '2'\n",
        "  col3 = col + '3'\n",
        "\n",
        "  X_tf[col2] = X_tf[col]**2\n",
        "  X_tf[col3] = X_tf[col]**3"
      ],
      "metadata": {
        "id": "YSqjhYV5SKnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** we experimented with the sklearn.preprocessing.polynomial() transformation but saw worse -RMSE performance than the approach outlined above."
      ],
      "metadata": {
        "id": "AtlScezG8lyz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improved Linear Regression"
      ],
      "metadata": {
        "id": "_VaJLzQIl9mI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf_lm = linear_model.LinearRegression(n_jobs=-1)\n",
        "\n",
        "scores = model_selection.cross_val_score(clf_lm,\n",
        "    X_tf, \n",
        "    y,\n",
        "    scoring=scoring,\n",
        "    cv=10\n",
        ")\n",
        "\n",
        "print(\"{} performance: {} (+/- {})\".format(scoring, round(scores.mean(), 2), round(scores.std(), 2)))"
      ],
      "metadata": {
        "id": "LOd1-agnc0M5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improved k-NN"
      ],
      "metadata": {
        "id": "6RsaCIgemARJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scale the data\n",
        "scaler = preprocessing.StandardScaler()\n",
        "scaler.fit(X_tf)\n",
        "X_tf_std = scaler.transform(X_tf)"
      ],
      "metadata": {
        "id": "OuD64Np_ZI_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_knn = neighbors.KNeighborsRegressor(\n",
        "    n_neighbors=5,\n",
        "    p=2,                     \n",
        "    metric='minkowski',\n",
        "    n_jobs=-1)\n",
        "\n",
        "# use the scaled version\n",
        "scores = model_selection.cross_val_score(clf_knn,\n",
        "    X_tf_std,\n",
        "    y,\n",
        "    scoring=scoring,\n",
        "    cv=10\n",
        ")\n",
        "\n",
        "print(\"{} performance: {} (+/- {})\".format(scoring, round(scores.mean(), 2), round(scores.std(), 2)))"
      ],
      "metadata": {
        "id": "yYBnqxFBc91a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improved Regression Tree"
      ],
      "metadata": {
        "id": "eNdkxAammBXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf_tree = tree.DecisionTreeRegressor(criterion='squared_error', \n",
        "                                      max_depth=3,\n",
        "                                      random_state=random_state)\n",
        "\n",
        "scores = model_selection.cross_val_score(clf_tree,\n",
        "    X_tf,\n",
        "    y,\n",
        "    scoring=scoring,\n",
        "    cv=10\n",
        ")\n",
        "\n",
        "print(\"{} performance: {} (+/- {})\".format(scoring, round(scores.mean(), 2), round(scores.std(), 2)))"
      ],
      "metadata": {
        "id": "xrrHuWFfmBff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improved Modeling Discussion\n",
        "Here we chose to do two kinds of feature engineering.\n",
        "\n",
        "First, we did manual feature engineering where we tried to inject domain expertise into our model. Our features are as follows:\n",
        "\n",
        "* `frequent_shopper`: codify people who shop >= 3 as \"frequent shoppers\" who may or may not buy more per transaction\n",
        "* `days_between_update`: days between 1st and last update\n",
        "* `female_shopping_residential`: female shoppers sending packages to residential addresses\n",
        "* `sqrt_Freq`: square root of Freq to make more linear\n",
        "* `all_three`: interaction term between Freq, last_update_days_ago,1st_update_days_ago \n",
        "\n",
        "Next, we did semi-automated feature engineering on the quantitative columns by raising each to the second and third power.\n",
        "\n",
        "**Note**: we did **not** incorporate the `preprocessing.Polynomial` approach that was demonstrated in class as we did not notice a better fit and we found the results to be less interpretable.\n",
        "\n",
        "---\n",
        "\n",
        "Given our scoring metric, 10 fold cv, and this feature engineering, our  **Logistic Regression** still has the best generalization performance in terms of bias (-113.85) but not necessarily the lowest variance (27.97). The bias is better than in part A, but our variance for Logistic Regression has increased. \n",
        "\n",
        "The kNN model improved in terms of bias (-126.85) and variance (25.13).\n",
        "\n",
        "The regression tree model got worse in terms of bias (-126.38) but the variance is relatively similar (26.45).\n",
        "\n",
        "The impact to logistic regression makes since as we have given it more parameters to fit on (which will decrease bias and increase variance). \n",
        "\n",
        "The kNN's overall fit got better, which is likely the result of our features improving its ability to determine similarity using distance. \n",
        "\n",
        "Finally, our regression tree got worse in terms of bias. We suspect this has to do with the manner in which splitting occurs. Decision trees are a \"greedy\" algorithm and therefore unstable in terms of splitting behavior (sometimes resulting in a different/worse tree)."
      ],
      "metadata": {
        "id": "XslyIYojmKa1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# c)\n",
        "Engage in parameter tuning to optimize the performance of linear regression, k-NN, and regression tree techniques. Use cross-validations with 10 folds to estimate the generalization performance. Present the results for each of the three techniques and discuss which one yields the best \n",
        "performance.\n",
        "\n",
        "part a is worth 35 points in total:\n",
        "* 10 points for correctly optimizing at least two parameters for linear regression model and improving the performance \n",
        "as much as possible - provide screenshots and explain what you are doing and the corresponding results \n",
        "* 10 points for correctly optimizing at least two parameters for linear k-NN model and improving the performance as \n",
        "much as possible - provide screenshots and explain what you are doing and the corresponding results\n",
        "* 10  points  for  correctly  optimizing  at  least  two  parameters  for  linear  regression  tree  model  and  improving  the \n",
        "performance as much as possible - provide screenshots and explain what you are doing and the corresponding results\n",
        "* 5 points for discussing which of the three models yields the best performance"
      ],
      "metadata": {
        "id": "E-OcErcfk7Tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# folds for nested cross-validation\n",
        "inner_cv = model_selection.KFold(n_splits=10, shuffle=True, random_state=random_state)\n",
        "\n",
        "outer_cv = model_selection.KFold(n_splits=10, shuffle=True, random_state=random_state) # outer is set to 10"
      ],
      "metadata": {
        "id": "VSbCaDCUnpXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuned Linear Regression"
      ],
      "metadata": {
        "id": "7XgwH2U7mW6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf_lr = linear_model.LinearRegression(n_jobs=-1)\n",
        "\n",
        "# two parameters to optimize\n",
        "fit_intercept_list = [True, False]\n",
        "positive_list = [True, False]\n",
        "\n",
        "# select the optimal parameters\n",
        "gs_lr = model_selection.GridSearchCV(\n",
        "    estimator=clf_lr,\n",
        "    param_grid=[{\n",
        "        'fit_intercept': fit_intercept_list,\n",
        "        'positive': positive_list}],\n",
        "    scoring=scoring,                                      \n",
        "    cv=inner_cv) \n",
        "\n",
        "gs_lr_fit = gs_lr.fit(X_tf,y)\n",
        "\n",
        "print(\"Non-nested Tuning:\")\n",
        "print(\"  Model: \", gs_lr_fit.best_estimator_)\n",
        "print(\"  Parameterization: \", gs_lr_fit.best_params_)\n",
        "print(\"  Non-nested CV {} score: \".format(scoring), round(gs_lr_fit.best_score_, 2))\n",
        "\n",
        "\n",
        "# generalization performance\n",
        "nested_gs_lr_fit= model_selection.cross_val_score(\n",
        "    gs_lr_fit, \n",
        "    X=X_tf, \n",
        "    y=y, \n",
        "    scoring=scoring,\n",
        "    cv=outer_cv)\n",
        "\n",
        "print(\"\\nNested Tuning:\")\n",
        "print(\"  Nested CV {} mean: \".format(scoring), round(nested_gs_lr_fit.mean(), 2), \" with st.dev (+/-): \", round(nested_gs_lr_fit.std(),2))"
      ],
      "metadata": {
        "id": "P2VD1Mxflqh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuned kNN"
      ],
      "metadata": {
        "id": "I0SS09wFmXks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf_knn = neighbors.KNeighborsRegressor(\n",
        "    p=2,                     \n",
        "    metric='minkowski',\n",
        "    n_jobs=-1)\n",
        "\n",
        "# two parameters to optimize\n",
        "weights_list = ['uniform', 'distance']\n",
        "n_neighbors_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
        "\n",
        "# select the optimal parameters\n",
        "gs_knn = model_selection.GridSearchCV(estimator=clf_knn,\n",
        "                                      param_grid=[{\n",
        "                                          'weights': weights_list,\n",
        "                                          'n_neighbors': n_neighbors_list}],\n",
        "                                      scoring=scoring,                                      \n",
        "                                      cv=inner_cv) \n",
        "\n",
        "\n",
        "gs_knn_fit = gs_knn.fit(X_tf_std,y)\n",
        "\n",
        "print(\"Non-nested Tuning:\")\n",
        "print(\"  Model: \", gs_knn_fit.best_estimator_)\n",
        "print(\"  Parameterization: \", gs_knn_fit.best_params_)\n",
        "print(\"  Non-nested CV {} score: \".format(scoring), round(gs_knn_fit.best_score_, 2))\n",
        "\n",
        "nested_gs_knn_fit= model_selection.cross_val_score(gs_knn_fit, \n",
        "                                                   X=X_tf_std, \n",
        "                                                   y=y, \n",
        "                                                   scoring=scoring,\n",
        "                                                   cv=outer_cv)\n",
        "\n",
        "\n",
        "# generalization performance\n",
        "print(\"\\nNested Tuning:\")\n",
        "print(\"  Nested CV {} mean: \".format(scoring), round(nested_gs_knn_fit.mean(), 2), \" with st.dev (+/-): \", round(nested_gs_knn_fit.std(),2))"
      ],
      "metadata": {
        "id": "Q0FaUB-dmXv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuned Regression Tree "
      ],
      "metadata": {
        "id": "EDz4pQl4mX4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a model\n",
        "clf_tree = tree.DecisionTreeRegressor(criterion='squared_error',\n",
        "                                      random_state=random_state)\n",
        "\n",
        "# two parameters to optimize\n",
        "min_samples_split_list = [2, 4, 6, 8, 10]\n",
        "max_depth_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, None]\n",
        "\n",
        "# select the optimal parameters \n",
        "gs_tree = model_selection.GridSearchCV(estimator=clf_tree,\n",
        "                                       param_grid=[{\n",
        "                                           'min_samples_split': min_samples_split_list,\n",
        "                                           'max_depth': max_depth_list}],\n",
        "                                       scoring=scoring,                                      \n",
        "                                       cv=inner_cv) \n",
        "\n",
        "gs_tree_fit = gs_tree.fit(X_tf,y)\n",
        "\n",
        "print(\"Non-nested Tuning:\")\n",
        "print(\"  Model: \", gs_tree_fit.best_estimator_)\n",
        "print(\"  Parameterization: \", gs_tree_fit.best_params_)\n",
        "print(\"  Non-nested CV {} score: \".format(scoring), round(gs_tree_fit.best_score_, 2))\n",
        "\n",
        "# generalization performance\n",
        "nested_gs_tree_fit = model_selection.cross_val_score(gs_tree_fit, \n",
        "                                                     X=X_tf, \n",
        "                                                     y=y, \n",
        "                                                     scoring=scoring,\n",
        "                                                     cv=outer_cv)\n",
        "\n",
        "print(\"nested Tuning:\")\n",
        "print(\"  Nested CV {} mean: \".format(scoring), round(nested_gs_tree_fit.mean(),2), \" with st.dev (+/-): \", round(nested_gs_tree_fit.std(),2))"
      ],
      "metadata": {
        "id": "bpByvcxPmYCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tuned Modeling Discussion\n",
        "Given our scoring metric, 10 fold cv, feature engineering, and parameter tuning, our **Logistic Regression** once again has the best generalization performance in terms of bias (-115.78) but once again has the lowest variance (20.91). \n",
        "\n",
        "Both of these values are better than the Logistic Regression from part A.\n",
        "\n",
        "Our bias is slightly worse in part C versus the Logistic Regression from part B, but our bias is better. This may ultimately be a better model as the prediction results would be more consistent while still remaining accurate."
      ],
      "metadata": {
        "id": "VYg0yyhjmYVu"
      }
    }
  ]
}